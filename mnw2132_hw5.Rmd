---
title: "p8105_hw5_mnw2132"
author: "Mary Williams"
date: "2024-11-15"
output: github_document
---
# Set up 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_libraries}
library(tidyverse)
library(ggplot2)
library(rvest)
library(broom)
```

# Problem 1
### Create the function
```{r}
#Create the function
birthday_data<-function(sample_size) {
  bday<- sample(1:365, sample_size, replace = TRUE)
  any(duplicated(bday))
}
```


```{r}
# Run the function
sim_bday= 
  tibble(
    sample_size = 2:50) %>%
  mutate(
    output_lists = map(.x = sample_size, ~rerun(10000, birthday_data(.x))), 
    estimate_df= map_dbl(output_lists, ~mean(unlist(.)))
) %>%
  select(sample_size, estimate_df)
```
### Graph the function
```{r}
sim_bday %>%
  ggplot(aes(x = sample_size, y = estimate_df)) +   geom_line(color = "light pink") +  geom_point(color = "red") + 
  labs(
    x = "Sample Size",
    y = "Probability of Duplicate Birthday",
    title = "Probability that â‰¥2 Share a Birthday") + 
theme_minimal()
```

# Problem 2
```{r}
set.seed(1)

#Create the function 
t_test = function(n = 30, mu = 0, sigma = 5) {
  sim_test = tibble(
    x = rnorm(n, mean = mu, sd = sigma))
  
  test_result = t.test(sim_test$x, mu = 0)
  tidy_result = tidy(test_result)
  
  tibble(
    mu_hat = mean(sim_test$x),
    p_value = tidy_result$p.value
  )
}

t_test(30)
```

```{r}
sim_results = map_dfr(c(0, 1, 2, 3, 4, 5, 6), function(mu) {
  map_dfr(1:5000, ~ t_test(n = 30, mu = mu, sigma = 5)) %>%
  mutate(true_mu = mu)
})
```

```{r}
results_df = bind_rows(sim_results)
```

## Plot 1: Plot of power vs True mean
```{r}
plot_1 = results_df %>%
  group_by(true_mu) %>%
  summarize(power = mean(p_value < 0.05))

ggplot(plot_1, aes(x = true_mu, y = power)) +
  geom_point(color = "red") + 
  geom_line(color = "pink") + 
  labs(x = "mu", 
       y = "Power", 
    title = "Power vs Effect Size")
```
The association between effect size and power is a positive association that follows a sigmoid shape. An effect size of 0 has a power of a little over 0, and then effect size of 6 approaches 1. 

## Plot 2: Plot of mu_hat vs true mean
```{r}
average_mu_hat = results_df %>%
  group_by(true_mu) %>%
  summarize(average_mu_hat = mean(mu_hat))

ggplot(average_mu_hat, aes(x = true_mu, y = average_mu_hat)) +
  geom_point(color = "red") +
  geom_line(color = "pink") +
  labs(x = "True Mean (Mu)",
       y = "Average mu_hat",
       title = "Average mu_hat vs True Mean")
```
Considering that the graph is a straight line from (0,0) to (6,6), this means that accross all of the simulations, the estimate of the sample mean is very close to the true mean. 
## Plot 3: Plot of mu_hat for regeected null hypothesis 
```{r}
rejected_df = results_df %>%
  filter(p_value < 0.05) %>%
  group_by(true_mu) %>%
  summarize (avg_rejected = mean(mu_hat))
```

```{r}
ggplot(rejected_df, aes(x = true_mu, y = avg_rejected)) +
       geom_point(color = "blue") +
         geom_line(color = "light blue") +
         labs(x = "True Mean",
              y = "Rejected Null",
              title = "Average estimate of mu hat for rejected null hypotheses")

```
For the rejected null graph, unlike the first graph, there is some variation in the point position. Specifically, for a true mean of 0, the rejection is negative, and for 1, the rejected null is about 2.5. This demonstrates that for smaller effect sizes or true means, there is a higher chance of failing to reject a false null hypothesis.

# Problem 3 

### Load info
```{r}
homicide_df = read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
summary(homicide_df)
```
The raw data includes information about the ID, information about the report like date, information about the victum like name, race, age and sex. And then location information like city, state and latitude, longitude, and disposition.  

### Create location variable
```{r}
homicide_df = homicide_df %>%
  mutate(location = paste(city, state, sep = ","))
```

### Homicides in each city
```{r}
city_summary = homicide_df %>%
  group_by(location) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

print(city_summary)
```
There are some cities that have less than 10 
### Baltimore, MD
```{r}
baltimore_data = city_summary %>%
  filter(location == "Baltimore,MD")

baltimore_prop = prop.test(baltimore_data$unsolved_homicides, baltimore_data$total_homicides)
baltimore_results = broom::tidy(baltimore_prop) %>%
  select(estimate, conf.low, conf.high)

```

### Estimate proportions 
```{r}
city_summary = city_summary %>%
  filter(total_homicides >2) %>%
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~prop.test(.x, .y) %>% 
    tidy()),
    estimate = map_dbl(prop_test, "estimate"),
    conf.low = map_dbl(prop_test, "conf.low"),
    conf.high = map_dbl(prop_test, "conf.high")
  )
```

### Plot proportion estimates for CI
```{r}
ggplot(city_summary, aes(x = reorder(location, estimate), y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "Location", y = "Proportion of Unsolved Homicides", title = "Proportion of Unsolved Homicides by each location") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
